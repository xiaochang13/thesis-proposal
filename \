In a statistical machine translation system, the task of translation is defined
to be the optimization problem of finding an English language sentence
$\mathbf{e}$, given a French language sentence $\mathbf{f}$:

\begin{equation} \label{basic}
\hat{\mathbf{e}} = \argmax_{\mathbf{e}} P(\mathbf{e}|\mathbf{f})
\end{equation}

\noindent where $P$ is a probability model that evaluates how good $\mathbf{e}$
is as a translation of $\mathbf{f}$. The model defines a story how $\mathbf{e}$
is generated by $\mathbf{f}$ and assigns a probability score to the story. The
optimization is carried out by a {\em decoding algorithm}, which is dependent on
the form of the probability model. Most decoding algorithms are dynamic
programming (DP) algorithms.

\vspace{5mm}

Broadly there are two kinds of models, {\em generative} and {\em
discriminative}. Generative models rewrite $P(\mathbf{e}|\mathbf{f})$ into a
product of conditional probabilities, which defines a step-by-step {\em
generative story} how $\mathbf{e}$ is generated from $\mathbf{f}$, while making
assumptions about the conditional distributions to reduce the number of
parameters in the model, thus extending its generalization power. A latent
variable or structure, $z$, is commonly included in the generative story. $z$
could be word alignment information in word-based model, phrase segmentation in
a phrase-based model, or parse trees in a syntax-based model.
$P(\mathbf{e}|\mathbf{f})$ is then a summation over the latent variable.

\begin{equation}
P(\mathbf{e}|\mathbf{f}) = \sum_z P(\mathbf{e},z|\mathbf{f})
\end{equation}

{\em Log-linear models} define a distribution in terms of arbitrary {\em
feature functions}. Let $\phi_1, \ldots, \phi_K$ be a set of $K$ feature
functions, a log-linear model is of the form

\begin{equation}
P(\mathbf{e}|\mathbf{f}) = \frac{\exp{(\sum_{k=1}^K \lambda_k
\phi_k(\mathbf{e},\mathbf{f}))}}{\sum_{\mathbf{e'}}
\exp{(\sum_{k=1}^K \lambda_k \phi_k(\mathbf{e'},\mathbf{f}))}}
\end{equation}

\noindent where $\lambda_k$ is weight for $\phi_k$ and can be tuned. If a latent
variable is involved, a log-linear model can be defined as
\cite{och02discriminative}

\begin{equation} \label{hiddenloglinear}
P(\mathbf{e}, z|\mathbf{f}) = \frac{\exp{(\sum_{k=1}^K \lambda_k
\phi_k(\mathbf{e},z,\mathbf{f}))}}{\sum_{\mathbf{e'},z'}
\exp{(\sum_{k=1}^K \lambda_k \phi_k(\mathbf{e'},z',\mathbf{f}))}}
\end{equation}

\vspace{5mm}

Translation quality is determined by both the model and the decoder. If the
model fails to assign a high probability score to a candidate translation that a
human deems good, we call this a {\em model error}. On the other hand, if the
model indeed ranks candidate translations according to human intuition, but the
decoder fails to find the top-ranked translation, we call this a {\em search
error} \cite{Brown:90}.

\vspace{5mm}

With a latent variable introduced into the model, the decoding problem becomes

\begin{equation}
\hat{\mathbf{e}} = \argmax_{\mathbf{e}} \sum_{z} P(\mathbf{e},z | \mathbf{f})
\end{equation}

This summation can be hard to compute, and the above optimization is often a
NP-complete problem. Therefore we often change our decoding goal and aim to
discover the best underlying structure of the model and English translation at
the same time.

\begin{equation}\label{latent}
\hat{\mathbf{e}} = \argmax_{\mathbf{e}} \max_z P(\mathbf{e},z | \mathbf{f})
\end{equation}

DP algorithms can be thought of as searching on weighted hypergraphs. The time
complexity of decoding is usually determined by how large a hypergraph we need
to explore. Indeed, some decoding problems are proved to be NP-complete
\cite{Knight99}, making an efficient algorithm unlikely.  Merging dynamic
programming states and pruning is a common to way to sidestep this problem,
which however deprives the decoder of its optimality.  Decoding is a trade-off
between quality and speed.

\section{Shortest-Path Problems on Weighted Hypergraphs}
\label{shortest}

Hypergraphs and semirings are a generalized framework to understand decoding
problems in machine translation. They establish the isomorphism between decoding
problems and shortest-path problems. We give an introduction to them in this
section. Many definitions in this section closely follow
\cite{huang2008advanced}.

\subsection{Hypergraphs and Semirings}

A semiring is an abstract algebraic structure that generalizes two operators
functioning on one set, with a distributive relationship. Addition and
multiplication of real numbers is a most common example.

\begin{definition} 
A {\em monoid} is a triple $\langle A, \otimes, \bar{1} \rangle$ where
$\otimes$ is a closed {\em associative} binary operator on the set $A$, and
$\bar{1}$ is the identity element for $\otimes$, i.e., $\forall a \in A$,
$a \otimes \bar{1} = \bar{1} \otimes a = a$. A monoid is {\em
commutative} if $\otimes$ is commutative.  \end{definition}

\begin{definition}
A {\em semiring} is a 5-tuple $\langle A, \oplus, \otimes, \bar{0},
\bar{1} \rangle$ such that

\begin{enumerate}
\item $(A, \oplus, \bar{0})$ is a commutative monoid.
\item $(A, \otimes, \bar{1})$ is a monoid.
\item $\otimes$ distributes over $\oplus$: $\forall a,b,c \in A$ 
\begin{align*}
(a \oplus b) \otimes c = (a \otimes c) \oplus (b \otimes c) \\
c \otimes (a \oplus b) = (c \otimes a) \oplus (c \otimes b)
\end{align*}
\item $\bar{0}$ is an {\em annihilator} for $\otimes$: $\forall a \in A,
\bar{0} \otimes a = a \otimes \bar{0} = \bar{0}$.
\end{enumerate}
\end{definition}

\begin{definition}
A semiring $\langle A, \oplus, \otimes, \bar{0}, \bar{1} \rangle$ is {\em
idempotent} if $\forall a \in A$, $a \oplus a = a$.
\end{definition}

The definition of idempotency is used to establish ordering in semirings.

\begin{definition}
Let $\langle A, \oplus, \otimes, \bar{0}, \bar{1} \rangle$ be an idempotent
semiring, then the relation $\le$ defined by
$$ (a \le b) \Leftrightarrow (a \oplus b = a) $$
is a partial ordering over $A$, called the {\em natural order} over $A$.
\end{definition}

\begin{definition}
An idempotent semiring $\langle A, \oplus, \otimes, \bar{0}, \bar{1} \rangle$ is
{\em totally ordered} if its natural order is a total order.
\end{definition}

(Examples of semirings)

We need a semiring to be totally-ordered to have a well-defined shortest-path
problem.

Hypergraphs are graphs whose edges are defined to have an ordered set of
sources and a single target. Hypergraphs have close relationships with And-Or
graphs and are a representation of dependencies or deductions: a conclusion is
reached only if all premises are satisfied.

\begin{definition}
A {\em hypergraph} is a pair $H = \langle V, E \rangle$, where $V$ is the set of
vertices and $E$ is the set of {\em hyperedges}. Each hyperedge $e \in E$ is
a pair $\langle h(e), T(e) \rangle$, where $h(e) \in V$ is its {\em head vertex}
and $T(e) \in V^*$ is an ordered list of {\em tail vertices}.
\end{definition}

We say a hypergraph is acyclic when there are no circular dependencies in it.

\begin{definition}
A hypergraph $\langle V, E\rangle$ is {\em acyclic} if its {\em graph
projection} $\langle V, E' \rangle$ is acyclic, where $$ E' = \{\langle u,v
\rangle \mid \exists e: u \in T(e) \land v = h(e) \} $$
\end{definition}

(Examples of hypergraphs)

(Give prior indications of what these concepts will be used for)

\begin{definition}
We denote the arity of a hyperedge by $|e| = |T(e)|$. $|H| = \max_{e \in E}
|e|$ is the {\em degree} of hypergraph $H$.  \end{definition}

\begin{definition}
For a vertex $v$, $\mathit{IN}(v) = \{e \in E \mid h(e) = V\}$ is the set of
{\em incoming hyperedges}, and $\mathit{OUT}(v) = \{e \in E \mid v \in T(e)\}$
is the set of {\em outgoing hyperedges}.  
\end{definition}

A path on a hypergraph, or a hyperpath, is a tree in the normal sense. It can be
defined recursively.

\begin{definition}
$d = \langle e, \Delta \rangle$ is a {\em derivation} or {\em hyperpath} on a
hypergraph $H = \langle V, E \rangle$, where $e \in E$ and $ \Delta$ is an
ordered list of {\em subderivations} such that

\begin{itemize}
\item $\Delta = \epsilon$ if $|e| = 0$,
\item $\Delta = \langle d_1, ..., d_{|e|} \rangle$ if $|e| > 0$, and $d_i$ for $1 \le
i \le |e|$ is a derivation of $T_i(e)$.
\end{itemize}
\end{definition}

We say a hyperedge $e \in d$ if it is recursively contained in any subderivation
of $d$. We also say a vertex $v \in d$ if it is the head of any hyperedge
contained in $d$.

\begin{definition}
$h(d) = h(e)$ is the head of derivation $d = \langle e, \Delta \rangle$.
\end{definition}

\begin{definition}
Given a vertex $v$ and a derivation $d = \langle e, \Delta \rangle$, $v \in
Y(d)$ is the {\em yield} of $d$, if and only if 
\begin{itemize} 
\item $|e| = 0$ and $h(e) = v$, or,
\item $\exists i \in \{1, \ldots, |e|\}: v \in Y(\Delta_i)$.
\end{itemize} 
\end{definition}

Given a hypergraph $H = \langle V, E \rangle$ and a semiring $\langle A, \oplus,
\otimes, \bar{0}, \bar{1} \rangle$, we can make a {\em weighted hypergraph} by
mapping each edge in $H$ to a weight from $A$: $w: E \mapsto A$. Then we can
define the weight of a derivation as the summation? of all the hyperedges the
derivation recursively contains.


\begin{definition}\label{defweight}
Given a derivation $d = \langle e, \Delta \rangle$, its weight is defined as
\begin{equation}\label{eqweight}
w(d) = w(e) \otimes \bigotimes_{i=1}^{|e|} w(\Delta_i)
\end{equation}
\end{definition}

Given an acyclic weighted hypergraph $H = \langle V, E \rangle$, a set of source
vertices $S \subseteq V$, and a target vertex $t \in V$, we can define
derivations under and above a vertex.

\begin{definition}
$D(v) = \{d = \langle e, \Delta \rangle \mid Y(d) \subseteq S \land h(d) = v\}$
is the set of derivations {\em under} vertex $v$.
\end{definition}

\begin{definition}
$\bar{D}(v) = \{d = \langle e, \Delta \rangle \mid Y(d) \subseteq S \cup \{v\}
\land h(d) = t\}$ is the set of derivations {\em above} vertex $v$.
\end{definition}

The shortest-path problem is then defined as finding the best derivation leading
from $S$ to $t$.

\begin{definition}
Given a vertex $v$, $\delta(v) = \bigoplus_{d \in D(v)} w(d)$ is the {\em best
weight} of $v$.
\end{definition}

Searching on a hypergraph can be difficult because the number of hyperedges in a
hypergraph can be exponential in the number of vertices, even if the hypergraph
is acyclic, unlike a graph where the number of edges is at most quadratic the
number of vertices. Most decoding problems are instances of shortest-path
problems.

\subsection{Properties of semirings}

A semiring can have two important properties that make finding the best
derivations easy: First, if a hypergraph is mapped to elements from a {\em
monotonic} semiring, the shortest-path problem decomposes into sub-problems and
thus satisfies the condition to use dynamic programming. Second, if the semiring
is also {\em superior}, we can apply a generalized version of Dijkstra's
algorithm \cite{knuth1977generalization} because superiority is equivalent to
``non-negativity" in the correctness proof of the original Dijkstra's algorithm.

\begin{definition}
A semiring $\langle A, \oplus, \otimes, \bar{0}, \bar{1} \rangle$ is {\em
monotonic} with respect to an ordering $\le$ if $\forall a, b, c \in A$ 
\begin{align*}
(a \le b) \Rightarrow (a \otimes c \le b \otimes c) \\
(a \le b) \Rightarrow (c \otimes a \le c \otimes b)
\end{align*}
\end{definition}

\begin{theorem}
A semiring is always monotonic with respect to its natural order. 
\end{theorem}

\begin{proof}
This directly follows from the distributivity of $\otimes$ over $\oplus$ in the
definition of semiring, i.e.
\begin{align*}
(a \le b) \Rightarrow (a \oplus b = a) \Rightarrow (c \otimes(a \oplus b)) = c
\otimes a \Rightarrow \\
(c \otimes a) \oplus (c \otimes b) = (c \otimes a)
\Rightarrow (c \otimes a) \le (c \otimes b)
\end{align*}
\noindent We have the same result if we multiply $c$ on the right. QED.
\end{proof} For the rest of this section we are
concerned only with monotonic semirings.


\begin{definition}
A semiring $\langle A, \oplus, \otimes, \bar{0}, \bar{1} \rangle$ is {\em
superior} with respect to a partial ordering $\le$ if $\forall a, b \in A$
\begin{align*}
a \le a \otimes b \\
b \le a \otimes b
\end{align*}
\end{definition}

\begin{theorem}
Given a superior semiring $\langle A, \oplus, \otimes, \bar{0}, \bar{1}
\rangle$, $\forall a \in A$ $$\bar{1} \le a \le \bar{0}$$
\end{theorem}
\begin{proof}
By superiority $\bar{1} \le \bar{1} \otimes a = a$, and $a \le \bar{0} \otimes a
= \bar{0}$. QED.
\end{proof}

Assuming an inside semiring, the definition inside and outside probabilities in
the language of hypergraphs is then:

\begin{align}
I(v) &= \bigoplus_{d \in D(v)} w(d) \\
O(v) &= \bigoplus_{d \in \bar{D}(v)} w(d) 
\end{align}

With these two values we can efficiently compute the sum of all paths that pass
through a particular vertex or hyperedge, which correspond to the probability of
the vertex or hyperedge. Such probabilities are used by EM training or pruning.

\begin{align}
\bigoplus_{d: v \in d} w(d) &= D(v) \otimes \bar{D}(v) \\
\bigoplus_{d: e \in d} w(d) &= w(e) \otimes \bar{D}(h(e)) \bigotimes_{v \in
T(e)} D(v) 
\end{align}

\subsection{Superior functions}

Our definition of the shortest path problem on hypergraphs here is slightly
different from Knuth's definition \cite{knuth1977generalization}, which defines
weights of derivations in terms of {\em superior functions}. 

\begin{definition}
Let $\mathbf{R}^+ = \mathbf{R} \cup \{+\infty$\}, a function $f:
\mathbf{R}^{+k} \mapsto \mathbf{R}^+$ is a {\em superior function} if

\begin{enumerate}
\item $ \forall i \in 1, ...,k \quad x_i \le x_i' \Rightarrow f(x_1, ..., x_i, ..., x_k)
\le f(x_1, ..., x_i', ..., x_k) $
\item $ f(x_1, ... ,x_k) \ge \max\{x_1, ..., x_k\} $
\end{enumerate}
\end{definition}

The definition of superior functions captures both monotonicity and superiority.
If we assign each hyperedge $e$ a superior function $f_e$, we can redefine the
weight of a derivation.

\begin{definition}
Given a derivation $d = \langle e, \Delta \rangle$, its weight in terms of superior
functions is
$$ w(d) = f_e(w(\Delta_1), ..., w(\Delta_{|e|})) $$
\end{definition}

For a derivation $\langle e, \emptyset \rangle$ on a source vertex, its weight
is then defined by a {\em nullary function}.

Our definition of derivation weight in terms of semirings is a special case of
superior functions. The right-hand side of Eq.~\ref{eqweight}, $w(e) \otimes
\bigotimes_{i=1}^{|e|} w(S_i)$, is a superior function if the $\oplus$ and
$\otimes$ operators used are from a superior semiring. The function's
superiority follows the superiority of the $\otimes$ operator. The function's
monotonicity follows from the monotonicity of the $\otimes$ operators over the
$\oplus$ operator. However, the semiring definition is also more general than
the superior function definition in the sense that it does not use the ordering
in $\mathbf{R}^+$.


\subsection{The Viterbi Algorithm}

If a hypergraph is acyclic, we can simply visit each vertex in topological order
and update the length of best hyperpath found so far. This is called the {\em
Viterbi algorithm} and the path found is sometimes called a Viterbi path.
Instances of the Viterbi algorithm include decoding in HMM, computation of
forward and backward probabilities in HMM, and computation of inside and outside
probabilities in PCFG. More generally, any dynamic programming algorithm is also
an instance of the Viterbi algorithm.

It should be noted that the Viterbi algorithm is not necessarily used on the
Viterbi semiring. For example, computation of inside probabilities is based on
the inside semiring. When the $\oplus$ operator does not dictate an ordering, as
in the case of the inside semiring, there is no notion of Viterbi paths.

\vspace{5mm}

The Viterbi algorithm runs in time $O(|V|+|E|)$.

\begin{algorithm}{\sc Viterbi}($H$)
\caption{The Viterbi Algorithm}
\begin{algorithmic}
\For{each vertex $v$ in $H$}
    \State $d(v) \leftarrow \bar{0}$
\EndFor
\For{each vertex $v$ in topological order}
    \For{each hyperedge $e$ in $\mathrm{IN}(v)$}
        \State $d(v) \oplus = w(e) \otimes \bigotimes_{i=1}^{|e|} d(T_i(e))$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}



\subsection{Generalized Dijkstra's Algorithm}

If a hypergraph is both monotonic and superior, the generalized Dijkstra's
Algorithm applies \cite{knuth1977generalization}. The idea is same with the
conventional Dijkstra's algorithm applied to a weighted graph, except that, on a
hypergraph, we trigger a vertex only when all its tail vertices have been popped
out of the priority queue.


\begin{algorithm}{\sc Dijkstra}($H$)
\caption{Generalized Dijkstra's Algorithm}
\begin{algorithmic}
\For{each vertex $v$ in $H$}
    \If{$v$ is a source vertex}
        \State $d(v) \leftarrow \bigoplus_{e \in \mathrm{IN}(v): |e| = 0} w(e)$
    \Else
        \State $d(v) \leftarrow \bar{0}$
    \EndIf
\EndFor
\State $Q \leftarrow V[H]$ is a priority queue prioritized by $d$-values
\For{each hyperedge $e$}
    \State $r[e] \leftarrow |e|$
\EndFor
\While{$Q \neq \emptyset$}
    \State $v \leftarrow$ {\sc Extract-Min}$(Q)$
    \For{each edge $e$ in $\mathrm{OUT}(v)$}
        \State $r[e] \leftarrow r[e] - 1$
        \If{$r[e] = 0$}
            \State $d(h(e)) \oplus = w(e) \otimes \bigotimes_{i=1}^{|e|} d(T_i(e))$
            \State {\sc Decrease-Key}$(Q, h(e))$
        \EndIf
    \EndFor
\EndWhile
\end{algorithmic}
\label{dijkstra}
\end{algorithm}

Generalized Dijkstra's algorithm runs in time $O\big((|V|+|E|)\log|V|\big)$ if
the priority queue is maintained as a binary heap, or
$O\big(|V|\log|V|+|E|\big)$ if a Fibonacci heap is used.

The correctness of Dijkstra's Algorithm lies in the loop invariant that whenever
a vertex $v$ is popped out of the priority queue, $d(v) = \delta(v)$.

Real problems often involve a large graph whose structure is not explicitly
saved. In practice Generalized Dijkstra's Algorithm is better implemented with
open and closed sets to keep track of vertices that still need to be explored
and vertices whose best weight is already computed, respectively. A closed set
is implicit in the implementation in Algorithm \ref{dijkstra}. The
implementation of Algorithm \ref{dijkstra2} still finds the shortest path if a
closed set is not used, but it is then a tree search algorithm and its running
time could be exponential because it examines every possible path on the
hypergraph shorter than the shortest path to the goal.


\begin{algorithm}{\sc Dijkstra}($H$)
\caption{Generalized Dijkstra's Algorithm implemented with open/closed sets}
\begin{algorithmic}
\State $Q \leftarrow \emptyset$ is a priority queue (open set) prioritized by $d$-values
\State $C \leftarrow \emptyset$ is a closed set of vertices
\For{each source vertex $v$ in $H$}
    \State $d(v) \leftarrow \bigoplus_{e \in \mathrm{IN}(v): |e| = 0} w(e)$
    \State {\sc Add}$(Q, v)$
\EndFor
\For{each hyperedge $e$}
    \State $r[e] \leftarrow |e|$
\EndFor
\While{$Q \neq \emptyset$}
    \State $v \leftarrow$ {\sc Extract-Min}$(Q)$
    \State {\sc Add}$(C, v)$
    \For{each edge $e$ in $\mathrm{OUT}(v)$}
        \State $r[e] \leftarrow r[e] - 1$
        \If{$r[e] = 0$ and $h(e) \notin C$}
            \State $d(h(e)) \oplus = w(e) \otimes \bigotimes_{i=1}^{|e|} d(T_i(e))$
            \If{$h(e) \in Q$}
                \State {\sc Decrease-Key}$(Q, h(e))$
            \Else
                \State {\sc Add}$(Q, h(e))$
            \EndIf
        \EndIf
    \EndFor
\EndWhile
\end{algorithmic}
\label{dijkstra2}
\end{algorithm}

If we have a goal vertex $t \in V$, Dijkstra can stop as soon as we reach the
goal vertex. This makes it faster than the Viterbi algorithm in problems where
$t$ is close to source vertices, although Viterbi has a lower big-O complexity.

\subsection{A* Algorithm}

In Generalized Dijkstra's Algorithm each vertex $v$ is prioritized by the weight
of its best derivation found so far, $d(v)$. A* adds a heuristic $h(v)$, and
estimate of the remaining cost to reach goal vertex $t$. The priority queue is
then order by $d(v) + h(v)$, a combination of current and remaining cost. Two
important properties of a heuristic are {\em admissibility} and {\em
consistency}. We extend these definitions to shortest path problems on
hypergraphs.

Assume we have a set of source vertices $S \subseteq V$ and target vertex $t \in
V$.

\begin{definition}
A heuristic $h$ is {\em admissible} if $\forall v \in V$
$$ h(v) \le \bigoplus_{d \in \bar{D}(v)} w(d) $$
\end{definition}

Admissibility says the heuristic never overestimates the future cost. 

\begin{definition}
A heuristic $h$ is {\em consistent} or {\em monotonic}\footnote{Not to be
confused with monotonicity of semirings.} if $\forall e = \langle v, T \rangle \in E$
$$ \forall u \in T, h(u) \le w(e) \otimes h(v) \bigotimes_{u' \in T -
\{v\}} \delta(u')$$ 
\end{definition}

Consistency is analogous to the non-negative weight requirement in Dijkstra's
algorithm. It says the value $d(v) \otimes h(v)$ never decreases when we move
along hyperedges.

\begin{theorem}
If a heuristic $h$ is consistent, and $h(t) = \bar{1}$, then $h$ is admissible.
\end{theorem}

\begin{proof}
The proof is an induction: for the goal vertex $t$, $h(t) = \bar{1} \le
\bigoplus_{d \in \bar{D}} w(d) = \bar{1}$. The induction step is for any vertex
$v$, for any $e = \langle u, T \rangle \in \mathrm{OUT}(v)$, by the definition
of consistency we have

\begin{align}
h(v) &\le w(e) \otimes h(u) \otimes \bigotimes_{u' \in T-\{v\}} \delta(u') \\
&\le w(e) \otimes \bigoplus_{d \in \bar{D}(u)} w(d) \otimes \bigotimes_{u' \in
T-\{v\}} \delta(u') \\
&\le \bigoplus_{d \in \bar{D}(u)} w(e) \otimes w(d) \otimes \bigotimes_{u' \in
T-\{v\}} \delta(u')
\end{align}

Then by the associativity of the $\oplus$ operator and the definition of $\le$
we have

\begin{equation} 
h(v) \oplus \bigoplus_{e = \langle u,T \rangle \in \mathrm{OUT}(v)} \bigoplus_{d \in
\bar{D}(u)} w(e) \otimes w(d) \otimes \bigotimes_{u' \in T-\{v\}} \delta(u') =
h(v)
\end{equation}

\noindent which means
\begin{align}
h(v) &\le \bigoplus_{e = \langle u,T \rangle \in \mathrm{OUT}(v)} \bigoplus_{d
\in \bar{D}(u)} w(e) \otimes w(d) \otimes \bigotimes_{u' \in T-\{v\}} \delta(u') \\
&= \bigoplus_{d \in \bar{D}(v)} w(d)
\end{align}

\noindent QED.

\end{proof}

A* search on hypergraph is optimal, i.e. it always finds the best hyperpath,
when the heuristic is consistent.

\section{Synchronous Context-Free Grammars}

Statistical machine translation has evolved from word-based models
\cite{Brown:93} to phrase-based models \cite{OchCL}, and then to syntax-based
models. Word-based models and phrase-based models are not designed to capture
word reordering information. Instead, what they do is simply try out different
possibilities and rely on the English language model to pick a fluent
translation candidate. They also cannot include any contextual information found
in non-contiguous phrases. Syntax-based translation models aim to use syntax to
capture such information in a principled way. Syntax-based models can be roughly
categorized into string-to-string \cite{ChiangCL, DekaiCL}, string-to-tree
\cite{galley-naacl04}, and tree-to-string models \cite{Liu-acl06}. Synchronous
context-free grammar is at the core of most syntax-based models.

Synchronous Context Free Grammar (SCFG) is a generalization of context free
grammar to generate bilingual sentences at the same time. Each grammar rule has
the form $$X \rightarrow \langle \gamma, \alpha, \sim, \rangle$$ where $\gamma$
is a string containing terminals and nonterminals in French, $\alpha$ in
English, and $\sim$ defines a one-to-one mapping between nonterminals in
$\gamma$ and $\alpha$.

An example of SCFG rules is shown in Figure~\ref{fig:scfg-example}. Rewriting
SCFG rules generates both a source-side sentence in Chinese and a target-side
sentence in English. The rewriting process also defines a derivation tree


\begin{figure}
%\begin{align*}
%X \rightarrow \textrm{wo, I} \\
%X \rightarrow \textrm{yu $X_1$ you $X_2$, have $X_2$ with $X_1$} \\
%\end{align*}
\label{fig:scfg-example}
\caption{An example of SCFG rules.}
\end{figure}

\subsection{Inducing SCFGs from Data}



\subsection{Decoding with SCFG}

A model defined by SCFG assumes a bilingual parse tree, or derivation, $d$ as
the hidden structure.  Following Eq. \ref{hiddenloglinear},

\begin{equation}
P(\mathbf{e}, d|\mathbf{f}) = \frac{\exp{(\sum_{k=1}^K \lambda_k
\phi_k(\mathbf{e},d,\mathbf{f}))}}{\sum_{\mathbf{e'},d'}
\exp{(\sum_{k=1}^K \lambda_k \phi_k(\mathbf{e'},d',\mathbf{f}))}}
\end{equation}

\noindent and the decoding problem is:

\begin{align}
\hat{\mathbf{e}} &= \argmax_{\mathbf{e}} \max_{d} P(\mathbf{e},d|\mathbf{f}) \\
&= \argmax_{\mathbf{e}} \max_{d} \sum_{k=1}^K \lambda_k \phi_k(\mathbf{e},d,\mathbf{f}) \\
& = \argmax_{\mathbf{e}} \max_{d} \sum_{e \in d} \sum_{k=1}^K \lambda_k \phi(e)
\end{align}

\noindent where $e$ is a hyperedge in parse tree $d$ and we are assuming that
all feature functions factorize over hyperedges. The hypergraph of this decoding
problem is determined by both $\mathbf{f}$ and the grammar, and it is sometimes
called a {\em parse forest} because it is a compact representation of all
possible parse trees. Decoding for SCFG is essentially parsing $\mathbf{f}$
monolingually with the French side of the SCFG, and generating $\mathbf{e}$ with
the English side of the grammar.

The widely used CYK algorithm for CFG parsing and decoding in syntax-based
translation is a special case of Viterbi algorithm running on a hypergraph
defined by the grammar. To see this, consider a context free grammar $G =
\langle N, T, R, S \rangle$ in Chomsky normal form (CNF), with nonterminal set
$N$, terminal set $T$, rule set $R$, and start symbol $S$. We use uppercase
letters for nonterminals and lowercase letters for terminals. Given a sentence
$\mathbf{f}$ of length $n$, parsing can be viewed as a series of deduction
steps. There are two kinds of deduction items, or vertices on the hypergraph.
First, dynamic programming states denoted by $[A,i,j]$, which says nonterminal
$A$ can be derived from sentence span $f_{i+1}^j$. Second, rule items denoted by
$(A \rightarrow BC)$ or $(A \rightarrow w)$. We have the following sets of
deductions, or hyperedges, shown in Fig. \ref{cyk}.

\begin{figure}
\begin{align*}
\mbox{\sc Initialization} &: \frac{(A \rightarrow f_{i+1})}{[A,i,i+1]} \\
\mbox{\sc Recursion} &: \frac{(A \rightarrow BC), [B,i,k], [C,k,j]}{[A,i,j]} \\
\mbox{\sc Goal} &: [S,0,n] \\ 
\end{align*}
\caption{Deduction for CYK parser}
\label{cyk}
\end{figure}

Since $1 \le i \le k \le j \le n$ there are $O(|R|)$ edges defined in the {\sc
Initialization} step and $O(|R| n^3)$ defined in the {\sc Recursion} step. Also,
the number of vertices is $O(|R| n^2)$. Recall that Viterbi algorithm runs in
$O(|V|+|E|)$, which in the special case of CYK is 
$$
O\left(|R|n^2 + (|R| n^3 +|R|n)\right) = O(|R| n^3) 
$$

When CFG rules are in general form $A \rightarrow \alpha$ where $\alpha \in (N
\cup T)^*$ is any combination of terminals and nonterminals, a possible dynamic
programming scheme is given by dot-rules used in the Earley parser \cite{Ear70},
whose items have the form $[A \rightarrow \alpha \cdot \beta,i,j]$, which says
the part of the rule before the dot, $\alpha$, has already been successfully
matched for $f_{i+1}^j$. The deduction steps are shown in Fig. \ref{earley}
\cite{chiang2006introduction}.

\begin{figure}
\begin{align*}
\mbox{\sc Initialization} &: \frac{(A \rightarrow \alpha)}{[A \rightarrow
\cdot \alpha, i, i]} \\
\mbox{\sc Scanning} &: \frac{[A \rightarrow \alpha \cdot f_{j+1} \beta, i, j]}{[A
\rightarrow \alpha f_{j+1} \cdot \beta, i, j+1]}\\
\mbox{\sc Completion} &: \frac{[A \rightarrow \alpha \cdot B \beta, i,j],
[B \rightarrow \gamma \cdot, j,k]}{[A \rightarrow \alpha B \cdot
\beta, i,k]} \\
\mbox{\sc Goal} &: [S \rightarrow \alpha \cdot, 0, n]
\end{align*}
\caption{Deduction for the simplified Earley parser}
\label{earley}
\end{figure}

Similarly by counting the number of vertices and hyperedges, the running time of
Earley parser is dominated by {\sc Completion} and it is $$O(|R|^2 n^3)$$
Although this seems worse than CYK, we have to keep in mind that converting a
CFG into CNF results in a quadratic blow-up of grammar size in the worst
case\cite{lange2009cnf}, which makes it no worse than CYK in practice.

The DP scheme we give above is a little bit simpler than the original version of
Earley's parser, shown in Fig. \ref{earley1}, which includes a {\sc Prediction}
step to reduce the number of dot-rules initialized. This however does not change
its worst case running time.

A* parsing is also possible for decoding with SCFGs.
\namecite{klein2001parsing} gives a general dual-agenda parsing algorithm, which
is essentially Generalized Dijkstra's algorithm applied to Earley style dynamic
programing described above. \namecite{klein-naacl03} discussed applying the A*
algorithm and admissible heuristics for parsing.

\begin{figure}
\begin{align*}
\mbox{\sc Initialization} &: \frac{(S \rightarrow \alpha)}{[S \rightarrow
\cdot \alpha, 0, 0]} \\
\mbox{\sc Prediction} &: \frac{(B \rightarrow \gamma), [A \rightarrow \alpha \cdot B
\beta, i,j]}{[B \rightarrow \cdot \gamma, j,j]}\\
\mbox{\sc Scanning} &: \frac{[A \rightarrow \alpha \cdot f_{j+1} \beta, i, j]}{[A
\rightarrow \alpha f_{j+1} \cdot \beta, i, j+1]}\\
\mbox{\sc Completion} &: \frac{[A \rightarrow \alpha \cdot B \beta, i,j],
[B \rightarrow \gamma \cdot, j,k]}{[A \rightarrow \alpha B \cdot
\beta, i,k]} \\
\mbox{\sc Goal} &: [S \rightarrow \alpha \cdot, 0, n]
\end{align*}
\caption{Deduction for the original Earley parser}
\label{earley1}
\end{figure}

\vspace{5mm}

We have assumed in the above discussion that features factorize over hyperedges.
This is true for many commonly used features but not for n-gram language models.
To maintain a correct dynamic programming algorithm we need to add boundary
words into the DP states, so that $[A,i,j]$ becomes $[A,i,j,u,v]$ for a bigram
language model where $u$ and $v$ are left and right boundary words.

Given a source-side binarized SCFG with target-side terminal set $T_e$
and nonterminal set $N$, the time complexity of decoding a sentence of
length $n$ with a $m$-gram language model is \cite{Venugopaletal2007}:
\[ 
O(n^{3} (|N| \cdot |T_e|^{2(m-1)})^{K})
\]
\noindent where $K$ is the maximum number of right-hand-side nonterminals, and
$T_e$ is the set of target-side terminals.  If we think of $|N|$ as a constant
and assume $|T_e|$ is proportional to $n$ in decoding a particular French
sentence. This running time can be also written as:
$$
O\big(n^{3+2K(m-1)}\big)
$$

\section{Speeding Up SCFG Decoding}

As shown in the last section, SCFG decoding with language model integration has
a prohibitive theoretical running time. A wide range of approaches has been
proposed to make SCFG-based machine translation practical.

\subsubsection{Restricted Grammar}

From a modelling perspective researchers try to define a restricted grammar in
the first place to reduce decoding complexity. Inverse Transduction Grammar
(ITG) proposed by \namecite{DekaiCL} has a form that is similar to CNF, limiting
each rule to have two nonterminals on the right-hand side. The grammar used in
hierarchical phrased-based translation \cite{ChiangCL} is a lexicalized grammar
where grammar rules have no more than two nonterminals on the right-hand side.
Moreover, adjacent nonterminals are banned to eliminate ambiguity at nonterminal
boundaries.

\subsubsection{Grammar Binarization}

Grammar rules with more than two nonterminals on the right-hand side can be
binarized to speed up decoding. In particular, language model integration can be
made faster by choosing a binarization that always group together source-side
nonterminals that are also adjacent on the target side. This is called
synchronous binarization \cite{ZHGK-naacl06}, and it reduces the theoretical
running time of $O(n^{3+2K(m-1)})$, as mentioned in the last section, to
$O(n^{3+2(m-1)})$

\subsubsection{Top-K Pruning}

In a shortest-path problem, we need to maintain one best score for each vertex
on the hypergraph $H = \langle V, E \rangle$.  This is computationally
prohibitive when the shortest path problem is defined on an exponentially large
DP hypergraph with respect to the original problem's size. We need to limit what
scores we can afford to maintain.

Generally, a way to approximate a large hypergraph is projecting the hypergraph
into a similar but smaller hypergraph. We first partition $V$ into $V_1,...,V_N$
using a mapping $m: V \mapsto \{V_1,\ldots,V_N\}$, and then copy hyperedges in
$E$ over using the partitions as new vertices.

\begin{definition}
$H' = \langle V', E' \rangle$ is the {\em hypergraph projection} of $H = \langle
V,E \rangle$ with respect to a partition of $V$, $\{V_1,\ldots,V_N\}$, where
\begin{align*}
V' &= \{1,\ldots,N \} \\
E' &= \{e' = \langle n, T' \rangle \mid \exists e = \langle v, T \rangle \in E:
v \in V_n \land \forall i \in \{1,\ldots,|e|\}, T_i \in V_{T'_i} \}
\end{align*}
\end{definition}

Since vertices on our hypergraphs are usually dynamic programming states with
multiple variables, hypergraph projection is often achieved by eliminating a
variable for DP states and collapsing related states.  For example, in parsing,
we can do a mapping $[A,i,j] \mapsto [i,j]$.

In top-K pruning, we maintain only $K$ best paths found for each partition,
which is called a {\em bin}. In an extreme case, let $K = \infty$ and make $V$
itself the only partition, this is equivalent to {\em best-first search}.
Pruning makes the algorithms we discussed in Section \ref{shortest} lose their
optimality.

Top-K pruning visits each bin topologically and generate a top-K list for each
bin. By doing this, we also lose the benefit of comparing items in a single
priority queue in original Dijkstra's or A* algorithm.

\subsubsection{Coarse-to-fine}

Another family of ideas to speed up SCFG decoding falls in to the coarse-to-fine
parsing framework, which is another example of hypergraph projection.  Given a
grammar $G = \langle N,T,R,S \rangle$, we first define a mapping $m: N \mapsto
N'$, where $N'$ is a smaller set of nonterminals. With this mapping we get a new
set of rules. For example, a rule $A \rightarrow BC$ will be mapped to $m(A)
\rightarrow m(B)m(C)$. This is equivalent to the DP state collapse $[A,i,j]
\mapsto [m(A),i,j]$.  More refined DP states decorated by higher order language
models can be approximated by those decorated by lower order language models
\cite{ZhangGildea-acl08}: $[A,i,j,u_1^m,v_1^m] \mapsto
[A,i,j,u_1^{m-1},v_1^{m-1}]$. A series of mappings can also be defined
\cite{charniak2006multi, petrov-EtAl:2006:COLACL} so we have a hierarchy of
increasingly coarse grammars.

Alternatively, we can directly prune the coarser hypergraph. One way
to do this is to prune vertices in the coarser hypergraph for whom the sum of
derivations passing it is smaller than a constant $C$, i.e.  
\begin{equation}
\bigoplus_{d: v \in d} w(d) \le C 
\end{equation} 
\noindent and then prune the vertices and hyperedges on the finer hypergraph
that map to the corresponding ones on the coarser hypergraph. This is adopted by
\cite{charniak2006multi}.

A coarse grammar can also be used for calculating a heuristic for A* search for
a fine grammar.  \namecite{ZhangGildea-acl08} used Viterbi outside probabilities
of states decorated by with bigram boundaries to guide search of a trigram state
space.  \namecite{pauls2009hierarchical} used the same heuristic and a hierarchy
of grammars, with each level of coarser grammar as a heuristic for the next
level finer grammar. One additional trick of \namecite{pauls2009hierarchical} is
defining deduction steps to link dynamic programming for heuristic computation
and that for shortest-path. Items for different levels of grammars also compete
in the same agenda.
