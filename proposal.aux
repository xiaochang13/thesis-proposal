\relax 
\@writefile{toc}{\contentsline {chapter}{Abstract}{ii}}
\citation{metropolis1949monte}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}MCMC algorithms}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:MCMC}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}MCMC}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Convergence property}{3}}
\citation{hastings1970monte}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Types of MCMC algorithms}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Metropolis-Hastings algorithm}{5}}
\citation{geman1984stochastic}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gibbs Sampling\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:gibbs}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Gibbs Sampling}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{7}}
\citation{yu-EtAl:2013:EMNLP}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}AMR parsing}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:amr}{{3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Forced decoding}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Forced decoding for AMR parsing}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Parameter update}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Conclusion}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Decipherment-based machine translation}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:decipher}{{4}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Decipherment-based machine translation}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Feature-based}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Reordering model}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Modeling sentence}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Conclusion}{14}}
\citation{collobert2011scratch}
\citation{chen2014fast,DurrettKlein2015}
\citation{bengio2003neural,mnih2007three}
\citation{devlin-EtAl:2014:P14-1,liu-EtAl:2014:P14-1,sutskever2014sequence,kalchbrenner2013recurrent}
\citation{Mikolov:2013}
\citation{mikolov2013distributed}
\citation{ling-EtAl:2015:NAACL-HLT}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Distributed representation learning}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:embedding}{{5}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{15}}
\citation{socher-EtAl:2013:ACL2013}
\citation{DBLP:journals/corr/LebretC15}
\citation{yu2015learning}
\citation{mikolov2013distributed}
\newlabel{fig:1a}{{5.1a}{17}}
\newlabel{sub@fig:1a}{{a}{17}}
\newlabel{fig:1b}{{5.1b}{17}}
\newlabel{sub@fig:1b}{{b}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Architecture of the skip-gram model.}\relax }}{17}}
\newlabel{fig:skip-gram}{{5.1}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Skip-gram model}{17}}
\newlabel{sec:skip}{{5.2}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Compositionality-aware skip-gram model}{18}}
\newlabel{sec:compose}{{5.3}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Phrase-level skip-gram model}{18}}
\newlabel{eq:phrase-skip}{{5.4}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Compositionality model}{19}}
\newlabel{eq:linear}{{5.7}{19}}
\citation{socher-EtAl:2013:ACL2013}
\citation{ling-EtAl:2015:NAACL-HLT}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Statistics of phrase length distributions (m/million)}\relax }}{20}}
\newlabel{tab:stats}{{5.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Output phrase embedding space}{20}}
\citation{collobert2011scratch}
\citation{wiki2010}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Examples of nearest neighbors using word2vec and using additional compositional model}\relax }}{21}}
\newlabel{tab:neigh}{{5.2}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments}{21}}
\newlabel{sec:experiment}{{5.4}{21}}
\citation{yu2015learning}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Comparison of word-level tasks, including word similarity, analogy.}\relax }}{22}}
\newlabel{tab:word-task}{{5.3}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Phrase nearest neighbors}{22}}
\citation{neelakantan-EtAl:2014:EMNLP2014}
\citation{chen2014fast}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {dependency parsing results on PTB using different embeddings}\relax }}{23}}
\newlabel{tab:dependency-task}{{5.4}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Word similarity and word analogy}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Dependency parsing}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}MCMC sampling}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Joint model}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Phrase independence variation}{25}}
\citation{Mitchell08vector-basedmodels}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces A joint model for phrase extraction and compositionality learning\relax }}{26}}
\newlabel{alg:joint}{{2}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Evaluation}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusion}{27}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Neural machine translation}{28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:nmt}{{6}{28}}
\bibstyle{urcsbiblio}
\bibdata{long}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{7}{29}}
\bibcite{geman1984stochastic}{{1}{1984}{{Geman and Geman}}{{}}}
\bibcite{hastings1970monte}{{2}{1970}{{Hastings}}{{}}}
\bibcite{metropolis1949monte}{{3}{1949}{{Metropolis and Ulam}}{{}}}
\bibcite{Mitchell08vector-basedmodels}{{4}{2008}{{Mitchell and Lapata}}{{}}}
\bibcite{yu-EtAl:2013:EMNLP}{{5}{2013}{{Yu et~al.}}{{Yu, Huang, Mi, and Zhao}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{30}}
